{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tf_Load_csv.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hari1331/Hari_DS/blob/master/Tf_Load_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB_aUAYpLFtx",
        "colab_type": "text"
      },
      "source": [
        "Install Tensorflow beta version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFW5U0_OLO4E",
        "colab_type": "text"
      },
      "source": [
        "Install tensorflow bets version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVshoUdnK58G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "7ada821b-092f-4702-d4ae-ce2bf62d60d4"
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0a20190603)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiWbU32FLfiz",
        "colab_type": "text"
      },
      "source": [
        "Import Numpy and tensorflow library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P59-KEZdLepl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0laBEozLqcY",
        "colab_type": "text"
      },
      "source": [
        "Get the Train and test date from Site storage.googleapis.com/tf.dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqGIrpiSLtAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "5590b6bd-f309-4e5e-e6f1-7f30a781ae49"
      },
      "source": [
        "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
        "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
        "\n",
        "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
        "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
            "32768/30874 [===============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n",
            "16384/13049 [=====================================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCZtILJcL8Ir",
        "colab_type": "text"
      },
      "source": [
        "**numpy.set_printoptions**. These options determine the way floating point numbers, arrays and other NumPy objects are displayed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O59tx48eL_Dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3b-H1sGMvGc",
        "colab_type": "text"
      },
      "source": [
        "Using Head function ,Read top 10 rows in the csv file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqYen5q6MvRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "57d6e947-8aeb-42e6-f33d-016a5b0d0f8f"
      },
      "source": [
        "!head {train_file_path}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\n",
            "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n",
            "1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\n",
            "1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\n",
            "1,female,35.0,1,0,53.1,First,C,Southampton,n\n",
            "0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\n",
            "0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\n",
            "1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\n",
            "1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\n",
            "1,female,4.0,1,1,16.7,Third,G,Southampton,n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7PIbO1VNPHp",
        "colab_type": "text"
      },
      "source": [
        "Each column of the CSV file will have a column name. The dataset's constructor automatically recognizes these column names. If the first line of the file you are using does not contain a column name, you need to pass the column name to the column_names parameter of the make_csv_dataset function via the list of strings.\n",
        "\n",
        "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n",
        "\n",
        "dataset = tf.data.experimental.make_csv_dataset(\n",
        "     ...,\n",
        "     column_names=CSV_COLUMNS,\n",
        "     ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heue04BmNvaa",
        "colab_type": "text"
      },
      "source": [
        "This example uses all the columns. If you need to ignore certain columns in the dataset, create a list containing the columns you need to use and pass it to the constructor's (optional) parameter select_columns.\n",
        "\n",
        "dataset = tf.data.experimental.make_csv_dataset(\n",
        "  ...,\n",
        "  select_columns = columns_to_use, \n",
        "  ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsa85JILODxw",
        "colab_type": "text"
      },
      "source": [
        "The columns that contain the values that the model needs to predict are explicitly specified. ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-oHVccJOFXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLUMN = 'survived'\n",
        "LABELS = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlryLOeZOSHX",
        "colab_type": "text"
      },
      "source": [
        "Now read the CSV data from the file and create a dataset\n",
        "For complete documentation, see tf.data.experimental.make_csv_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSDhH-tuOYKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset(file_path):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=12, \n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True)\n",
        "  return dataset\n",
        "\n",
        "raw_train_data = get_dataset(train_file_path)\n",
        "raw_test_data = get_dataset(test_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrMMC7BROqVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "864af3d0-b046-41f4-9332-93bd3eebb9b2"
      },
      "source": [
        "examples, labels = next(iter(raw_train_data)) \n",
        "print(\"EXAMPLES: \\n\", examples, \"\\n\")\n",
        "print(\"LABELS: \\n\", labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EXAMPLES: \n",
            " OrderedDict([('sex', <tf.Tensor: id=326, shape=(12,), dtype=string, numpy=\n",
            "array([b'male', b'male', b'female', b'female', b'female', b'male',\n",
            "       b'female', b'male', b'male', b'male', b'female', b'male'],\n",
            "      dtype=object)>), ('age', <tf.Tensor: id=318, shape=(12,), dtype=float32, numpy=\n",
            "array([36., 28., 35., 32., 28., 35., 27., 17., 44., 29., 39., 71.],\n",
            "      dtype=float32)>), ('n_siblings_spouses', <tf.Tensor: id=324, shape=(12,), dtype=int32, numpy=array([1, 0, 0, 0, 1, 0, 1, 0, 2, 1, 1, 0], dtype=int32)>), ('parch', <tf.Tensor: id=325, shape=(12,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], dtype=int32)>), ('fare', <tf.Tensor: id=323, shape=(12,), dtype=float32, numpy=\n",
            "array([ 15.55 ,   7.775, 512.329,  13.   ,  24.   , 512.329,  13.858,\n",
            "         8.663,  90.   ,   7.046,  83.158,  49.504], dtype=float32)>), ('class', <tf.Tensor: id=320, shape=(12,), dtype=string, numpy=\n",
            "array([b'Third', b'Third', b'First', b'Second', b'Second', b'First',\n",
            "       b'Second', b'Third', b'First', b'Third', b'First', b'First'],\n",
            "      dtype=object)>), ('deck', <tf.Tensor: id=321, shape=(12,), dtype=string, numpy=\n",
            "array([b'unknown', b'unknown', b'unknown', b'unknown', b'unknown', b'B',\n",
            "       b'unknown', b'unknown', b'C', b'unknown', b'E', b'unknown'],\n",
            "      dtype=object)>), ('embark_town', <tf.Tensor: id=322, shape=(12,), dtype=string, numpy=\n",
            "array([b'Southampton', b'Southampton', b'Cherbourg', b'Southampton',\n",
            "       b'Cherbourg', b'Cherbourg', b'Cherbourg', b'Southampton',\n",
            "       b'Queenstown', b'Southampton', b'Cherbourg', b'Cherbourg'],\n",
            "      dtype=object)>), ('alone', <tf.Tensor: id=319, shape=(12,), dtype=string, numpy=\n",
            "array([b'n', b'y', b'y', b'y', b'n', b'y', b'n', b'y', b'n', b'n', b'n',\n",
            "       b'y'], dtype=object)>)]) \n",
            "\n",
            "LABELS: \n",
            " tf.Tensor([0 1 1 1 1 1 1 0 0 0 1 0], shape=(12,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNkvVHOjPXAV",
        "colab_type": "text"
      },
      "source": [
        "**Classification data**\n",
        "Some of the columns in the CSV data are classified columns. That is, these columns can only take values in a limited set.\n",
        "Create a *tf.feature_column.indicator_column* collection using the *tf.feature_column API*, with each *tf.feature_column.indicator_column* corresponding to a **categorized column**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPs9qWgbPsGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORIES = {\n",
        "    'sex': ['male', 'female'],\n",
        "    'class' : ['First', 'Second', 'Third'],\n",
        "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
        "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
        "    'alone' : ['y', 'n']\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro4inPvrP29B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf4imNjaP6K7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "fd021d65-62b4-4a7d-dd3d-e50013f66fdd"
      },
      "source": [
        "categorical_columns"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
              " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('First', 'Second', 'Third'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
              " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
              " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Cherbourg', 'Southhampton', 'Queenstown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
              " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('y', 'n'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KkARIlEQSpL",
        "colab_type": "text"
      },
      "source": [
        "**Continuous data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "486aiWj6QVZh",
        "colab_type": "text"
      },
      "source": [
        "**Continuous data needs to be standardized**\n",
        "Write a function to normalize these values and then transform those values into 2D tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NFLE9q-Qga9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_continuous_data(mean, data):\n",
        "  data = tf.cast(data, tf.float32) * 1/(2*mean)\n",
        "  return tf.reshape(data, [-1, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqWeaeqzQsOZ",
        "colab_type": "text"
      },
      "source": [
        "Now create a collection of numeric columns. The tf.feature_columns.numeric_column API uses the normalizer_fn parameter. Use functools.partial when passing arguments, and functools.partial consists of functions that are normalized using the mean of each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIhmf_g7QtWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEANS = {\n",
        "    'age' : 29.631308,\n",
        "    'n_siblings_spouses' : 0.545455,\n",
        "    'parch' : 0.379585,\n",
        "    'fare' : 34.385399\n",
        "}\n",
        "\n",
        "numerical_columns = []\n",
        "\n",
        "for feature in MEANS.keys():\n",
        "  num_col = tf.feature_column.numeric_column(feature, normalizer_fn=functools.partial(process_continuous_data, MEANS[feature]))\n",
        "  numerical_columns.append(num_col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qJKTBjoQyTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "4c61942e-0f63-400b-bd3b-3898fd3978d0"
      },
      "source": [
        "numerical_columns"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NumericColumn(key='age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x7f473d4db2f0>, 29.631308)),\n",
              " NumericColumn(key='n_siblings_spouses', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x7f473d4db2f0>, 0.545455)),\n",
              " NumericColumn(key='parch', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x7f473d4db2f0>, 0.379585)),\n",
              " NumericColumn(key='fare', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function process_continuous_data at 0x7f473d4db2f0>, 34.385399))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_SRtRo6Q5GT",
        "colab_type": "text"
      },
      "source": [
        "**Create a pre-processing layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_yiX8bWQ6bf",
        "colab_type": "text"
      },
      "source": [
        "Add the set of these two feature columns and pass it to tf.keras.layers.DenseFeatures to create an input layer for preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB6CGsEeRBvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numerical_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75gZV91ARYc4",
        "colab_type": "text"
      },
      "source": [
        "**Building a model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQgQj3d4RZpV",
        "colab_type": "text"
      },
      "source": [
        "Build tf.keras.Sequential from preprocessing_layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSt-9F0TRfyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7A--51JRkeG",
        "colab_type": "text"
      },
      "source": [
        "**Training, assessment and prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT3iAYibRp5k",
        "colab_type": "text"
      },
      "source": [
        "It is now possible to instantiate and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87TW472iRulz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = raw_train_data.shuffle(500)\n",
        "test_data = raw_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzAA3cP1RyPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "f957165a-be6d-40c7-aa99-2884d7ad78bf"
      },
      "source": [
        "model.fit(train_data, epochs=20)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0729 15:09:53.149945 139945016321920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0729 15:09:53.169103 139945016321920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4215: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "W0729 15:09:53.170221 139945016321920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/feature_column/feature_column_v2.py:4270: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "53/53 [==============================] - 2s 35ms/step - loss: 0.5214 - accuracy: 0.7435\n",
            "Epoch 2/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4276 - accuracy: 0.8228\n",
            "Epoch 3/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.4113 - accuracy: 0.8297\n",
            "Epoch 4/20\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.4010 - accuracy: 0.8523\n",
            "Epoch 5/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8573\n",
            "Epoch 6/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.8574\n",
            "Epoch 7/20\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3808 - accuracy: 0.8576\n",
            "Epoch 8/20\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3756 - accuracy: 0.8588\n",
            "Epoch 9/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3712 - accuracy: 0.8609\n",
            "Epoch 10/20\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3670 - accuracy: 0.8608\n",
            "Epoch 11/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3631 - accuracy: 0.8684\n",
            "Epoch 12/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3595 - accuracy: 0.8684\n",
            "Epoch 13/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3560 - accuracy: 0.8693\n",
            "Epoch 14/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8699\n",
            "Epoch 15/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3490 - accuracy: 0.8701\n",
            "Epoch 16/20\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3455 - accuracy: 0.8644\n",
            "Epoch 17/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8667\n",
            "Epoch 18/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.8669\n",
            "Epoch 19/20\n",
            "53/53 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8686\n",
            "Epoch 20/20\n",
            "53/53 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f473d3fe198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsJDqfysR5zY",
        "colab_type": "text"
      },
      "source": [
        "When the model training is complete, you can check the accuracy on the test set test_data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-Fgbi5YR97O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "62a8faff-4c70-4300-f163-5b490f3ebe72"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     22/Unknown - 1s 26ms/step - loss: 0.4386 - accuracy: 0.8106\n",
            "\n",
            "Test Loss 0.4386001236059449, Test Accuracy 0.810606062412262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRdMU1DbSBkS",
        "colab_type": "text"
      },
      "source": [
        "Infer a label for one or more batches using tf.keras.Model.predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRXBtkaOSFbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "445b06a5-7dfa-4c24-8177-ee385aebea20"
      },
      "source": [
        "predictions = model.predict(test_data)\n",
        "for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
        "  print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n",
        "        \" | Actual outcome: \",\n",
        "        (\"SURVIVED\" if bool(survived) else \"DIED\"))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted survival: 86.33%  | Actual outcome:  DIED\n",
            "Predicted survival: 99.82%  | Actual outcome:  SURVIVED\n",
            "Predicted survival: 12.17%  | Actual outcome:  DIED\n",
            "Predicted survival: 15.31%  | Actual outcome:  DIED\n",
            "Predicted survival: 30.53%  | Actual outcome:  DIED\n",
            "Predicted survival: 12.43%  | Actual outcome:  DIED\n",
            "Predicted survival: 27.71%  | Actual outcome:  DIED\n",
            "Predicted survival: 4.65%  | Actual outcome:  DIED\n",
            "Predicted survival: 75.25%  | Actual outcome:  SURVIVED\n",
            "Predicted survival: 89.47%  | Actual outcome:  DIED\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}